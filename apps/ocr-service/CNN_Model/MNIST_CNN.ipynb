{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3332948e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, models \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import sys\n",
    "from tensorflow.keras import layers, models # type: ignore\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1445f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the CNN model (or load a pre-trained one)\n",
    "def build_and_train_model():\n",
    "    (x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.mnist.load_data() #Load Data\n",
    "    x_train_full = x_train_full.astype('float32') / 255.0 #Normalize //from 0 to 1\n",
    "    x_test = x_test.astype('float32') / 255.0 #Normalize\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "    x_val = x_val.reshape(-1, 28, 28, 1)\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "    # Build the CNN model\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.6), #to prevent overfitting\n",
    "        layers.Dense(10, activation='softmax')  # 10 classes for digits 0-9\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "    # Save the model\n",
    "    model.save('attendance_digit_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7deb752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model \n",
    "#build_and_train_model()\n",
    "model = load_model('./attendance_digit_model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35780fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    img = Image.open(path)\n",
    "    #.convert('L')           # Convert to grayscale\n",
    "    #img = img.resize((28, 28))                    # Resize to 28x28\n",
    "    # img_array = np.array(img) / 255.0             # Normalize\n",
    "    # img_array = 1 - img_array                     # Invert colors if white digit on black bg\n",
    "    # img_array = img_array.reshape(1, 28, 28, 1)   # Add batch and channel dims\n",
    "    return img\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    filenames = []\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            path = os.path.join(folder, filename)\n",
    "            img = Image.open(path).convert(\"L\")\n",
    "            # img = np.array(img) / 255.0  # normalize\n",
    "            # img = 1 - img  # invert\n",
    "            # img = img.reshape(28, 28, 1)\n",
    "            images.append(img)\n",
    "            filenames.append(filename)\n",
    "    return np.stack(images), filenames  # <- stack into one tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "307510ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "id_cell_001.png ➤ 0\n",
      "id_cell_002.png ➤ 0\n",
      "id_cell_003.png ➤ 0\n",
      "id_cell_004.png ➤ 0\n",
      "id_cell_005.png ➤ 0\n",
      "id_cell_006.png ➤ 0\n",
      "id_cell_007.png ➤ 0\n",
      "id_cell_008.png ➤ 0\n",
      "id_cell_009.png ➤ 0\n",
      "id_cell_010.png ➤ 0\n",
      "id_cell_011.png ➤ 0\n",
      "id_cell_012.png ➤ 0\n",
      "id_cell_013.png ➤ 0\n",
      "id_cell_014.png ➤ 0\n",
      "id_cell_015.png ➤ 0\n",
      "id_cell_016.png ➤ 0\n",
      "id_cell_017.png ➤ 1\n",
      "id_cell_018.png ➤ 1\n",
      "id_cell_019.png ➤ 1\n",
      "id_cell_020.png ➤ 1\n",
      "id_cell_021.png ➤ 1\n",
      "id_cell_022.png ➤ 1\n",
      "id_cell_023.png ➤ 1\n",
      "id_cell_024.png ➤ 7\n",
      "id_cell_025.png ➤ 9\n",
      "id_cell_026.png ➤ 1\n",
      "id_cell_027.png ➤ 1\n",
      "id_cell_028.png ➤ 1\n",
      "id_cell_029.png ➤ 1\n",
      "id_cell_030.png ➤ 1\n",
      "id_cell_031.png ➤ 1\n",
      "id_cell_032.png ➤ 1\n",
      "id_cell_033.png ➤ 2\n",
      "id_cell_034.png ➤ 2\n",
      "id_cell_035.png ➤ 2\n",
      "id_cell_036.png ➤ 2\n",
      "id_cell_037.png ➤ 2\n",
      "id_cell_038.png ➤ 2\n",
      "id_cell_039.png ➤ 2\n",
      "id_cell_040.png ➤ 2\n",
      "id_cell_041.png ➤ 2\n",
      "id_cell_042.png ➤ 2\n",
      "id_cell_043.png ➤ 2\n",
      "id_cell_044.png ➤ 2\n",
      "id_cell_045.png ➤ 2\n",
      "id_cell_046.png ➤ 2\n",
      "id_cell_047.png ➤ 2\n",
      "id_cell_048.png ➤ 2\n",
      "id_cell_049.png ➤ 2\n",
      "id_cell_050.png ➤ 3\n",
      "id_cell_051.png ➤ 3\n",
      "id_cell_052.png ➤ 3\n",
      "id_cell_053.png ➤ 3\n",
      "id_cell_054.png ➤ 2\n",
      "id_cell_055.png ➤ 3\n",
      "id_cell_056.png ➤ 3\n",
      "id_cell_057.png ➤ 3\n",
      "id_cell_058.png ➤ 2\n",
      "id_cell_059.png ➤ 2\n",
      "id_cell_060.png ➤ 2\n",
      "id_cell_061.png ➤ 2\n",
      "id_cell_062.png ➤ 3\n",
      "id_cell_063.png ➤ 3\n",
      "id_cell_064.png ➤ 3\n",
      "id_cell_065.png ➤ 4\n",
      "id_cell_066.png ➤ 4\n",
      "id_cell_067.png ➤ 9\n",
      "id_cell_068.png ➤ 4\n",
      "id_cell_069.png ➤ 4\n",
      "id_cell_070.png ➤ 4\n",
      "id_cell_071.png ➤ 4\n",
      "id_cell_072.png ➤ 8\n",
      "id_cell_073.png ➤ 4\n",
      "id_cell_074.png ➤ 4\n",
      "id_cell_075.png ➤ 4\n",
      "id_cell_076.png ➤ 9\n",
      "id_cell_077.png ➤ 9\n",
      "id_cell_078.png ➤ 4\n",
      "id_cell_079.png ➤ 4\n",
      "id_cell_080.png ➤ 9\n",
      "id_cell_081.png ➤ 5\n",
      "id_cell_082.png ➤ 5\n",
      "id_cell_083.png ➤ 5\n",
      "id_cell_084.png ➤ 5\n",
      "id_cell_085.png ➤ 5\n",
      "id_cell_086.png ➤ 5\n",
      "id_cell_087.png ➤ 5\n",
      "id_cell_088.png ➤ 5\n",
      "id_cell_089.png ➤ 5\n",
      "id_cell_090.png ➤ 5\n",
      "id_cell_091.png ➤ 5\n",
      "id_cell_092.png ➤ 5\n",
      "id_cell_093.png ➤ 5\n",
      "id_cell_094.png ➤ 5\n",
      "id_cell_095.png ➤ 5\n",
      "id_cell_096.png ➤ 5\n",
      "id_cell_097.png ➤ 6\n",
      "id_cell_098.png ➤ 6\n",
      "id_cell_099.png ➤ 6\n",
      "id_cell_100.png ➤ 6\n",
      "id_cell_101.png ➤ 6\n",
      "id_cell_102.png ➤ 6\n",
      "id_cell_103.png ➤ 6\n",
      "id_cell_104.png ➤ 6\n",
      "id_cell_105.png ➤ 0\n",
      "id_cell_106.png ➤ 6\n",
      "id_cell_107.png ➤ 6\n",
      "id_cell_108.png ➤ 6\n",
      "id_cell_109.png ➤ 6\n",
      "id_cell_110.png ➤ 6\n",
      "id_cell_111.png ➤ 6\n",
      "id_cell_112.png ➤ 6\n",
      "id_cell_113.png ➤ 7\n",
      "id_cell_114.png ➤ 7\n",
      "id_cell_115.png ➤ 7\n",
      "id_cell_116.png ➤ 7\n",
      "id_cell_117.png ➤ 7\n",
      "id_cell_118.png ➤ 7\n",
      "id_cell_119.png ➤ 7\n",
      "id_cell_120.png ➤ 7\n",
      "id_cell_121.png ➤ 7\n",
      "id_cell_122.png ➤ 7\n",
      "id_cell_123.png ➤ 7\n",
      "id_cell_124.png ➤ 7\n",
      "id_cell_125.png ➤ 7\n",
      "id_cell_126.png ➤ 7\n",
      "id_cell_127.png ➤ 7\n",
      "id_cell_128.png ➤ 7\n",
      "id_cell_129.png ➤ 8\n",
      "id_cell_130.png ➤ 8\n",
      "id_cell_131.png ➤ 8\n",
      "id_cell_132.png ➤ 8\n",
      "id_cell_133.png ➤ 8\n",
      "id_cell_134.png ➤ 8\n",
      "id_cell_135.png ➤ 8\n",
      "id_cell_136.png ➤ 8\n",
      "id_cell_137.png ➤ 8\n",
      "id_cell_138.png ➤ 8\n",
      "id_cell_139.png ➤ 8\n",
      "id_cell_140.png ➤ 8\n",
      "id_cell_141.png ➤ 8\n",
      "id_cell_142.png ➤ 8\n",
      "id_cell_143.png ➤ 8\n",
      "id_cell_144.png ➤ 8\n",
      "id_cell_145.png ➤ 9\n",
      "id_cell_146.png ➤ 9\n",
      "id_cell_147.png ➤ 9\n",
      "id_cell_148.png ➤ 9\n",
      "id_cell_149.png ➤ 9\n",
      "id_cell_150.png ➤ 8\n",
      "id_cell_151.png ➤ 9\n",
      "id_cell_152.png ➤ 9\n",
      "id_cell_153.png ➤ 9\n",
      "id_cell_154.png ➤ 9\n",
      "id_cell_155.png ➤ 9\n",
      "id_cell_156.png ➤ 9\n",
      "id_cell_157.png ➤ 9\n",
      "id_cell_158.png ➤ 9\n",
      "id_cell_159.png ➤ 9\n",
      "id_cell_160.png ➤ 9\n",
      "id_cell_161.png ➤ 2\n",
      "id_cell_162.png ➤ 2\n",
      "id_cell_163.png ➤ 2\n",
      "id_cell_164.png ➤ 2\n",
      "id_cell_165.png ➤ 2\n",
      "id_cell_166.png ➤ 2\n",
      "id_cell_167.png ➤ 2\n",
      "id_cell_168.png ➤ 2\n",
      "id_cell_169.png ➤ 4\n",
      "id_cell_170.png ➤ 4\n",
      "id_cell_171.png ➤ 4\n",
      "id_cell_172.png ➤ 4\n",
      "id_cell_173.png ➤ 4\n",
      "id_cell_174.png ➤ 4\n",
      "id_cell_175.png ➤ 4\n",
      "id_cell_176.png ➤ 4\n",
      "id_cell_177.png ➤ 6\n",
      "id_cell_178.png ➤ 9\n",
      "id_cell_179.png ➤ 6\n",
      "id_cell_180.png ➤ 9\n",
      "id_cell_181.png ➤ 6\n",
      "id_cell_182.png ➤ 9\n",
      "id_cell_183.png ➤ 6\n",
      "id_cell_184.png ➤ 9\n",
      "mnist_image_1_label_2.png ➤ 2\n",
      "mnist_ready_digit[1].png ➤ 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Load all images\n",
    "image_folder = \"../images/final_clean_cells/\"\n",
    "images, filenames = load_images_from_folder(image_folder)\n",
    "\n",
    "# # Function to display images\n",
    "# def display_images(images, titles, rows, cols, figsize=(15, 5)):\n",
    "#     fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "#     for i, (img, title) in enumerate(zip(images, titles)):\n",
    "#         ax = axes[i//cols, i%cols] if rows > 1 else axes[i]\n",
    "#         ax.imshow(img, cmap='gray')\n",
    "#         ax.set_title(title)\n",
    "#         ax.axis('off')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Display first 5 images\n",
    "# print(\"First 5 images:\")\n",
    "# display_images(images[:5], [f\"Image {i+1}\" for i in range(5)], 1, 5)\n",
    "\n",
    "# # Display last 5 images\n",
    "# print(\"\\nLast 5 images:\")\n",
    "# display_images(images[-5:], [f\"Image {len(images)-4+i}\" for i in range(5)], 1, 5)\n",
    "\n",
    "# Predict on the full batch\n",
    "prediction = model.predict(images)\n",
    "predicted_digits = np.argmax(prediction, axis=1)\n",
    "\n",
    "# Output results\n",
    "for fname, digit in zip(filenames, predicted_digits):\n",
    "    print(f\"{fname} ➤ {digit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33185f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_and_train_model3():\n",
    "    # 1) LOAD & PREPARE\n",
    "    (x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    x_train_full = x_train_full.astype('float32') / 255.0\n",
    "    x_test       = x_test.astype('float32')       / 255.0\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "    # add channel dim\n",
    "    x_train = x_train[..., np.newaxis]\n",
    "    x_val   = x_val[...,   np.newaxis]\n",
    "    x_test  = x_test[...,  np.newaxis]\n",
    "\n",
    "    # 2) SET UP AUGMENTER\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        shear_range=0.1\n",
    "    )\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # 3) GENERATE ONE “EPOCH” WORTH OF AUGMENTED SAMPLES\n",
    "    batch_size = 32\n",
    "    steps = len(x_train) // batch_size\n",
    "    X_aug_list, y_aug_list = [], []\n",
    "    aug_iter = datagen.flow(x_train, y_train, batch_size=batch_size, shuffle=False)\n",
    "    for _ in range(steps):\n",
    "        xb, yb = next(aug_iter)\n",
    "        X_aug_list.append(xb)\n",
    "        y_aug_list.append(yb)\n",
    "    X_aug = np.vstack(X_aug_list)\n",
    "    y_aug = np.hstack(y_aug_list)\n",
    "\n",
    "    # 4) COMBINE RAW + AUGMENTED & SHUFFLE\n",
    "    X_comb = np.concatenate([x_train, X_aug], axis=0)\n",
    "    y_comb = np.concatenate([y_train, y_aug], axis=0)\n",
    "    X_comb, y_comb = shuffle(X_comb, y_comb, random_state=42)\n",
    "\n",
    "    # 5) BUILD MODEL\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(28,28,1)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # 6) CALLBACKS\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr  = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "    # 7) TRAIN on combined dataset\n",
    "    model.fit(\n",
    "        X_comb, y_comb,\n",
    "        batch_size=64,\n",
    "        epochs=10,\n",
    "        validation_data=(x_val, y_val),\n",
    "        callbacks=[early_stop, reduce_lr]\n",
    "    )\n",
    "\n",
    "    # 8) SAVE & EVAL\n",
    "    model.save('attendance_digit_model3.h5')\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ea400f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fares\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1908/1908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 39ms/step - accuracy: 0.8554 - loss: 0.4928\n",
      "Epoch 2/10\n",
      "\u001b[1m1908/1908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 32ms/step - accuracy: 0.9747 - loss: 0.0839\n",
      "Epoch 3/10\n",
      "\u001b[1m1908/1908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 32ms/step - accuracy: 0.9811 - loss: 0.0609\n",
      "Epoch 4/10\n",
      "\u001b[1m1908/1908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 32ms/step - accuracy: 0.9853 - loss: 0.0481\n",
      "Epoch 5/10\n",
      "\u001b[1m1908/1908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 32ms/step - accuracy: 0.9894 - loss: 0.0369\n",
      "Epoch 6/10\n",
      "\u001b[1m1908/1908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 32ms/step - accuracy: 0.9908 - loss: 0.0306\n",
      "Epoch 7/10\n",
      "\u001b[1m1908/1908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 32ms/step - accuracy: 0.9927 - loss: 0.0242\n",
      "Epoch 8/10\n",
      "\u001b[1m1908/1908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 33ms/step - accuracy: 0.9937 - loss: 0.0200\n",
      "Epoch 9/10\n",
      "\u001b[1m1908/1908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 34ms/step - accuracy: 0.9941 - loss: 0.0178\n",
      "Epoch 10/10\n",
      "\u001b[1m1908/1908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 36ms/step - accuracy: 0.9940 - loss: 0.0182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "build_and_train_model_on_combined_data(r'C:\\Users\\Fares\\Downloads\\GitHub\\Scanvas\\apps\\ocr-service\\CNN_Model')\n",
    "# Load the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
