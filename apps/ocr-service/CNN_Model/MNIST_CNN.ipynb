{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3332948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import sys\n",
    "from tensorflow.keras import layers, models # type: ignore\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1445f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the CNN model (or load a pre-trained one)\n",
    "def build_and_train_model():\n",
    "    (x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.mnist.load_data() #Load Data\n",
    "    x_train_full = x_train_full.astype('float32') / 255.0 #Normalize //from 0 to 1\n",
    "    x_test = x_test.astype('float32') / 255.0 #Normalize\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "    x_val = x_val.reshape(-1, 28, 28, 1)\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "    # Build the CNN model\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.6), #to prevent overfitting\n",
    "        layers.Dense(10, activation='softmax')  # 10 classes for digits 0-9\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "    # Save the model\n",
    "    model.save('attendance_digit_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7deb752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model \n",
    "#build_and_train_model()\n",
    "model = load_model('./attendance_digit_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35780fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    img = Image.open(path)\n",
    "    #.convert('L')           # Convert to grayscale\n",
    "    #img = img.resize((28, 28))                    # Resize to 28x28\n",
    "    # img_array = np.array(img) / 255.0             # Normalize\n",
    "    # img_array = 1 - img_array                     # Invert colors if white digit on black bg\n",
    "    # img_array = img_array.reshape(1, 28, 28, 1)   # Add batch and channel dims\n",
    "    return img\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    filenames = []\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            path = os.path.join(folder, filename)\n",
    "            img = Image.open(path).convert(\"L\")\n",
    "            # img = np.array(img) / 255.0  # normalize\n",
    "            # img = 1 - img  # invert\n",
    "            # img = img.reshape(28, 28, 1)\n",
    "            images.append(img)\n",
    "            filenames.append(filename)\n",
    "    return np.stack(images), filenames  # <- stack into one tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "307510ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "id_cell_001.png ➤ 0\n",
      "id_cell_002.png ➤ 0\n",
      "id_cell_003.png ➤ 0\n",
      "id_cell_004.png ➤ 0\n",
      "id_cell_005.png ➤ 8\n",
      "id_cell_006.png ➤ 0\n",
      "id_cell_007.png ➤ 0\n",
      "id_cell_008.png ➤ 0\n",
      "id_cell_009.png ➤ 0\n",
      "id_cell_010.png ➤ 0\n",
      "id_cell_011.png ➤ 0\n",
      "id_cell_012.png ➤ 8\n",
      "id_cell_013.png ➤ 0\n",
      "id_cell_014.png ➤ 0\n",
      "id_cell_015.png ➤ 0\n",
      "id_cell_016.png ➤ 0\n",
      "id_cell_017.png ➤ 1\n",
      "id_cell_018.png ➤ 1\n",
      "id_cell_019.png ➤ 1\n",
      "id_cell_020.png ➤ 1\n",
      "id_cell_021.png ➤ 1\n",
      "id_cell_022.png ➤ 1\n",
      "id_cell_023.png ➤ 1\n",
      "id_cell_024.png ➤ 7\n",
      "id_cell_025.png ➤ 7\n",
      "id_cell_026.png ➤ 1\n",
      "id_cell_027.png ➤ 1\n",
      "id_cell_028.png ➤ 1\n",
      "id_cell_029.png ➤ 1\n",
      "id_cell_030.png ➤ 1\n",
      "id_cell_031.png ➤ 1\n",
      "id_cell_032.png ➤ 1\n",
      "id_cell_033.png ➤ 2\n",
      "id_cell_034.png ➤ 2\n",
      "id_cell_035.png ➤ 2\n",
      "id_cell_036.png ➤ 2\n",
      "id_cell_037.png ➤ 2\n",
      "id_cell_038.png ➤ 2\n",
      "id_cell_039.png ➤ 2\n",
      "id_cell_040.png ➤ 2\n",
      "id_cell_041.png ➤ 2\n",
      "id_cell_042.png ➤ 2\n",
      "id_cell_043.png ➤ 2\n",
      "id_cell_044.png ➤ 2\n",
      "id_cell_045.png ➤ 2\n",
      "id_cell_046.png ➤ 2\n",
      "id_cell_047.png ➤ 2\n",
      "id_cell_048.png ➤ 2\n",
      "id_cell_049.png ➤ 3\n",
      "id_cell_050.png ➤ 3\n",
      "id_cell_051.png ➤ 3\n",
      "id_cell_052.png ➤ 3\n",
      "id_cell_053.png ➤ 3\n",
      "id_cell_054.png ➤ 3\n",
      "id_cell_055.png ➤ 3\n",
      "id_cell_056.png ➤ 3\n",
      "id_cell_057.png ➤ 3\n",
      "id_cell_058.png ➤ 3\n",
      "id_cell_059.png ➤ 3\n",
      "id_cell_060.png ➤ 3\n",
      "id_cell_061.png ➤ 2\n",
      "id_cell_062.png ➤ 3\n",
      "id_cell_063.png ➤ 3\n",
      "id_cell_064.png ➤ 3\n",
      "id_cell_065.png ➤ 4\n",
      "id_cell_066.png ➤ 4\n",
      "id_cell_067.png ➤ 4\n",
      "id_cell_068.png ➤ 4\n",
      "id_cell_069.png ➤ 4\n",
      "id_cell_070.png ➤ 4\n",
      "id_cell_071.png ➤ 4\n",
      "id_cell_072.png ➤ 7\n",
      "id_cell_073.png ➤ 4\n",
      "id_cell_074.png ➤ 4\n",
      "id_cell_075.png ➤ 4\n",
      "id_cell_076.png ➤ 7\n",
      "id_cell_077.png ➤ 4\n",
      "id_cell_078.png ➤ 4\n",
      "id_cell_079.png ➤ 4\n",
      "id_cell_080.png ➤ 4\n",
      "id_cell_081.png ➤ 5\n",
      "id_cell_082.png ➤ 5\n",
      "id_cell_083.png ➤ 5\n",
      "id_cell_084.png ➤ 5\n",
      "id_cell_085.png ➤ 5\n",
      "id_cell_086.png ➤ 5\n",
      "id_cell_087.png ➤ 5\n",
      "id_cell_088.png ➤ 5\n",
      "id_cell_089.png ➤ 5\n",
      "id_cell_090.png ➤ 5\n",
      "id_cell_091.png ➤ 5\n",
      "id_cell_092.png ➤ 5\n",
      "id_cell_093.png ➤ 5\n",
      "id_cell_094.png ➤ 5\n",
      "id_cell_095.png ➤ 5\n",
      "id_cell_096.png ➤ 5\n",
      "id_cell_097.png ➤ 8\n",
      "id_cell_098.png ➤ 8\n",
      "id_cell_099.png ➤ 6\n",
      "id_cell_100.png ➤ 6\n",
      "id_cell_101.png ➤ 8\n",
      "id_cell_102.png ➤ 8\n",
      "id_cell_103.png ➤ 8\n",
      "id_cell_104.png ➤ 5\n",
      "id_cell_105.png ➤ 6\n",
      "id_cell_106.png ➤ 6\n",
      "id_cell_107.png ➤ 6\n",
      "id_cell_108.png ➤ 5\n",
      "id_cell_109.png ➤ 6\n",
      "id_cell_110.png ➤ 8\n",
      "id_cell_111.png ➤ 6\n",
      "id_cell_112.png ➤ 6\n",
      "id_cell_113.png ➤ 7\n",
      "id_cell_114.png ➤ 7\n",
      "id_cell_115.png ➤ 7\n",
      "id_cell_116.png ➤ 7\n",
      "id_cell_117.png ➤ 7\n",
      "id_cell_118.png ➤ 7\n",
      "id_cell_119.png ➤ 7\n",
      "id_cell_120.png ➤ 7\n",
      "id_cell_121.png ➤ 7\n",
      "id_cell_122.png ➤ 7\n",
      "id_cell_123.png ➤ 7\n",
      "id_cell_124.png ➤ 7\n",
      "id_cell_125.png ➤ 7\n",
      "id_cell_126.png ➤ 7\n",
      "id_cell_127.png ➤ 7\n",
      "id_cell_128.png ➤ 7\n",
      "id_cell_129.png ➤ 8\n",
      "id_cell_130.png ➤ 8\n",
      "id_cell_131.png ➤ 8\n",
      "id_cell_132.png ➤ 8\n",
      "id_cell_133.png ➤ 8\n",
      "id_cell_134.png ➤ 8\n",
      "id_cell_135.png ➤ 8\n",
      "id_cell_136.png ➤ 8\n",
      "id_cell_137.png ➤ 8\n",
      "id_cell_138.png ➤ 8\n",
      "id_cell_139.png ➤ 8\n",
      "id_cell_140.png ➤ 8\n",
      "id_cell_141.png ➤ 8\n",
      "id_cell_142.png ➤ 8\n",
      "id_cell_143.png ➤ 8\n",
      "id_cell_144.png ➤ 8\n",
      "id_cell_145.png ➤ 7\n",
      "id_cell_146.png ➤ 7\n",
      "id_cell_147.png ➤ 3\n",
      "id_cell_148.png ➤ 3\n",
      "id_cell_149.png ➤ 2\n",
      "id_cell_150.png ➤ 8\n",
      "id_cell_151.png ➤ 7\n",
      "id_cell_152.png ➤ 8\n",
      "id_cell_153.png ➤ 3\n",
      "id_cell_154.png ➤ 7\n",
      "id_cell_155.png ➤ 7\n",
      "id_cell_156.png ➤ 7\n",
      "id_cell_157.png ➤ 7\n",
      "id_cell_158.png ➤ 7\n",
      "id_cell_159.png ➤ 7\n",
      "id_cell_160.png ➤ 7\n",
      "id_cell_161.png ➤ 2\n",
      "id_cell_162.png ➤ 2\n",
      "id_cell_163.png ➤ 2\n",
      "id_cell_164.png ➤ 2\n",
      "id_cell_165.png ➤ 2\n",
      "id_cell_166.png ➤ 2\n",
      "id_cell_167.png ➤ 2\n",
      "id_cell_168.png ➤ 2\n",
      "id_cell_169.png ➤ 4\n",
      "id_cell_170.png ➤ 4\n",
      "id_cell_171.png ➤ 4\n",
      "id_cell_172.png ➤ 4\n",
      "id_cell_173.png ➤ 4\n",
      "id_cell_174.png ➤ 4\n",
      "id_cell_175.png ➤ 4\n",
      "id_cell_176.png ➤ 4\n",
      "id_cell_177.png ➤ 5\n",
      "id_cell_178.png ➤ 9\n",
      "id_cell_179.png ➤ 5\n",
      "id_cell_180.png ➤ 9\n",
      "id_cell_181.png ➤ 6\n",
      "id_cell_182.png ➤ 9\n",
      "id_cell_183.png ➤ 5\n",
      "id_cell_184.png ➤ 9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Load all images\n",
    "image_folder = \"../images/final_clean_cells/\"\n",
    "images, filenames = load_images_from_folder(image_folder)\n",
    "\n",
    "# # Function to display images\n",
    "# def display_images(images, titles, rows, cols, figsize=(15, 5)):\n",
    "#     fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "#     for i, (img, title) in enumerate(zip(images, titles)):\n",
    "#         ax = axes[i//cols, i%cols] if rows > 1 else axes[i]\n",
    "#         ax.imshow(img, cmap='gray')\n",
    "#         ax.set_title(title)\n",
    "#         ax.axis('off')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Display first 5 images\n",
    "# print(\"First 5 images:\")\n",
    "# display_images(images[:5], [f\"Image {i+1}\" for i in range(5)], 1, 5)\n",
    "\n",
    "# # Display last 5 images\n",
    "# print(\"\\nLast 5 images:\")\n",
    "# display_images(images[-5:], [f\"Image {len(images)-4+i}\" for i in range(5)], 1, 5)\n",
    "\n",
    "# Predict on the full batch\n",
    "prediction = model.predict(images)\n",
    "predicted_digits = np.argmax(prediction, axis=1)\n",
    "\n",
    "# Output results\n",
    "for fname, digit in zip(filenames, predicted_digits):\n",
    "    print(f\"{fname} ➤ {digit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33185f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
